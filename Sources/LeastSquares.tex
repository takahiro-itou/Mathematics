%%  -*-  coding:utf-8  -*-
\documentclass[12pt]{jsarticle}

\input{Preambles}

\def\error{\varepsilon}
\def\sumdata{\sum_{i=1}^{N}}

\title{最小二乗法}

\begin{document}

\maketitle

\section{概要}
ある観測データの組 $(x, y)$ は、モデル関数 $\function{f}{x}$ と
誤差 $\varepsilon$ の和で
\begin{equation}
  y = \function{f}{x} + \error
\end{equation}
と表せるとする。

$N$ 組の観測データ $(x_i, y_i)$ が与えられたとき、
各 $x_i$ についてモデルから得られる予測値
$\function{f}{x_i}$ と、実際の測定値 $y_i$ の誤差 $\error_i$ つまり
\begin{equation}
  \error_i := \left| \function{f}{x_i} - y_i \right|
\end{equation}
の和を最小にすることを考えたい。
しかし、絶対値があると後々の計算が大変なので、
実際には、誤差の二乗和

\begin{equation}
  \Delta := \frac{1}{2} \sumdata \error_i^2 = \frac{1}{2} \sumdata
  \left( \function{f}{x_i} - y_i \right)^2 \label{eq:least_square}
\end{equation}

を最小にするようなモデル $\function{f}{x}$ を求める。
なお、係数 $\frac{1}{2}$ は、後の計算の時に、
多少計算が楽になるのを見越して付けているだけで、本質ではない。

以下のセクションでは、簡単なものから複雑なものまで、
具体的に求めてみる。

\section{線形回帰 (直線) の場合}

モデル関数は
\begin{equation}
  f \left( x \right) = a x + b
\end{equation}
であるから、式 (\ref{eq:least_square}) は
\begin{equation}
  \Delta = \frac{1}{2} \sumdata \error_i = \frac{1}{2}
  \sumdata \left( a x_i + b - y_i \right) ^2
\end{equation}
となる。これを最小化する $a, b$ を求めればよい。
これを $a, b$ に関する二次関数と見れば、この関数が最小値を取る点では
$a$ および $b$ についての偏微分がゼロであるから、

\begin{eqnarray*}
  \partd{}{a} \Delta &=& 0, \\
  \partd{}{b} \Delta &=& 0
\end{eqnarray*}
を連立して解けばよい。実際に計算してみると

\begin{eqnarray*}
  \partd{}{a} \Delta = \sumdata \left(
  a x_i + b - y_i \right) x_i
  = a \sumdata x_i^2 +b \sumdata x_i - \sumdata x_i y_i
  &=& 0, \\
  \partd{}{b} \Delta = \sumdata \left(
  a x_i + b - y_i \right) 1
  = a \sumdata x_i + b \sumdata 1 - \sumdata y_i, &=& 0,
\end{eqnarray*}

すなわち

\begin{equation}
  \left\{
  \begin{array}{ll}
    \left( \sumdata x_i^2 \right) a + \left( \sumdata x_i \right) b
    &= \sumdata x_i y_i, \\
    \left( \sumdata x_i \right) a + \left( \sumdata 1 \right) b
    &= \sumdata y_i,
  \end{array}
  \right.
\end{equation}

を解けばよい。行列を使って書けば

\begin{equation}
  \begin{bmatrix}
    \sumdata x_i^2, & \sumdata x_i \\
    \sumdata x_i,   & \sumdata 1
  \end{bmatrix}
  \begin{bmatrix}
    a \\
    b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sumdata x_i y_i \\
    \sumdata y_i
  \end{bmatrix}
\end{equation}

よって、
\begin{equation}
  Det := \left( \sumdata x_i^2 \right) \left( \sumdata 1 \right)
  - \left( \sumdata x_i \right)^2
\end{equation}
とおけば、

\begin{eqnarray}
  \begin{bmatrix}
    a \\
    b
  \end{bmatrix}
  &=&
  \begin{bmatrix}
    \sumdata x_i^2, & \sumdata x_i \\
    \sumdata x_i,    & \sumdata 1
  \end{bmatrix} ^{-1}
  \begin{bmatrix}
    \sumdata x_i y_i \\
    \sumdata y_i
  \end{bmatrix} \nonumber \\
  &=&
  \frac{1}{Det}
  \begin{bmatrix}
    \sumdata 1,     & - \sumdata x_i, \\
    - \sumdata x_i, & \sumdata x_i^2
  \end{bmatrix}
  \begin{bmatrix}
    \sumdata x_i y_i \\
    \sumdata y_i
  \end{bmatrix}
\end{eqnarray}

さらに $\sumdata 1 = N$ であるから、最終的に

\begin{eqnarray}
  a &=& \frac{N \sumdata x_i y_i -
    \left( \sumdata x_i \right) \left( \sumdata y_i \right)
  }{N \sumdata x_i^2 - \left( \sumdata x_i \right)^2}, \nonumber \\
  b &=& \frac{
    \left( \sumdata x_i^2 \right) \left( \sumdata y_i \right)
    -
    \left( \sumdata x_i y_i \right) \left( \sumdata x_i \right)
  }{N \sumdata x_i^2 - \left( \sumdata x_i \right)^2}, \nonumber \\
\end{eqnarray}

\section{一般的な多項式の場合}

モデル関数は
\begin{equation}
  f \left( x \right) = a_0 + a_1 x^1 + \cdots + a_{n-1} x^{n-1} + a_n x^n
\end{equation}

であるから、式 (\ref{eq:least_square}) は

\begin{equation}
  \Delta = \frac{1}{2}
  \sumdata \left( a_0 + a_1 x_i + + a_2 x_i^2
  + \cdots + a_{n-1} x_i^{n-1} + a_n x_i^n - y_i \right) ^2
\end{equation}
となる。先ほどと同様に $a_0, a_a, \ldots, a_n$ で偏微分すると

\begin{eqnarray*}
  \partd{}{a_0} \Delta &=& \sumdata \left(
  a_0 + a_1 x_i + \cdots a_n x_i^n  - y_i  \right) 1, \nonumber \\
  \partd{}{a_1} \Delta &=& \sumdata \left(
  a_0 + a_1 x_i + \cdots a_n x_i^n  - y_i  \right) x_i, \nonumber \\
  \partd{}{a_2} \Delta &=& \sumdata \left(
  a_0 + a_1 x_i + \cdots a_n x_i^n  - y_i  \right) x_i^2, \nonumber \\
  \cdots \nonumber \\
  \partd{}{a_n} \Delta &=& \sumdata \left(
  a_0 + a_1 x_i + \cdots a_n x_i^n  - y_i  \right) x_n^2, \nonumber \\
\end{eqnarray*}

この各式がゼロとおいて、移項して行列の形に纏めると

\begin{equation}
  \begin{bmatrix}
    \sumdata x_i^0, & \sumdata x_i^1, & \sumdata x_i^2,
    & \cdots, & \sumdata x_i^n \\
    \sumdata x_i^1, & \sumdata x_i^2, & \sumdata x_i^3,
    & \cdots, & \sumdata x_i^{n+1} \\
    \sumdata x_i^2, & \sumdata x_i^3, & \sumdata x_i^4,
    & \cdots, & \sumdata x_i^{n+2} \\
    \vdots, & \vdots, & \vdots, & \ddots, & \vdots, \\
    \sumdata x_i^n, & \sumdata x_i^{n+1} & \sumdata x_i^{n+2},
    & \cdots, & \sumdata x_i^{2n}
  \end{bmatrix}
  \begin{bmatrix}
    a_0 \\ a_1 \\ a_2 \\ \vdots \\ a_n
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sumdata x_i^0 y_i \\
    \sumdata x_i^1 y_i \\
    \sumdata x_i^2 y_i \\
    \vdots \\
    \sumdata x_i^n y_i
  \end{bmatrix}
\end{equation}

後はこれに掃き出し法や LR 分解など、
連立一次方程式の解法アルゴリズムを適用すれば良い。

\end{document}
